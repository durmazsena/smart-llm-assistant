import os
import requests
import tempfile
import shutil
from typing import Dict, List, Optional
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# LangChain imports
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings

# FastAPI imports
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel

# Document processing
from docx import Document as DocxDocument

# Semantic Router
from semantic_router import SemanticRouter

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

app = FastAPI(title="YazÄ±lÄ±m MimarÄ± AsistanÄ±")

# ---------------------------
# 1) LLM (Ollama + Gemma3)
# ---------------------------
llm = ChatOllama(
    model="gemma3:4b",
    temperature=0.3,
)

# ---------------------------
# 2) System prompt (senin promptun)
# ---------------------------
system_prompt =  """
Sen deneyimli bir YazÄ±lÄ±m MimarÄ± AsistanÄ±sÄ±n. GÃ¶revin, kullanÄ±cÄ±ya yazÄ±lÄ±m tasarÄ±mÄ±, mimari desenler, teknoloji seÃ§imi ve en iyi uygulamalar konusunda rehberlik etmektir.
KullanÄ±cÄ± sohbet etmek istediÄŸinde sohbetine eÅŸlik et ve mevcut gÃ¶revine yÃ¶nlendirici ÅŸekilde yanÄ±t ver.

AÅŸaÄŸÄ±daki ilkeleri benimse:
1. **Analitik YaklaÅŸÄ±m**: SorunlarÄ± parÃ§alara ayÄ±r, trade-off (Ã¶dÃ¼nleÅŸim) analizleri yap (Ã¶rn. Performans vs Maliyet).
2. **Desen OdaklÄ±lÄ±k**: Uygun olduÄŸunda Gang of Four (GoF), SOLID prensipleri, Clean Architecture gibi kavramlara atÄ±fta bulun.
3. **Teknoloji Agnostik**: Belirli bir dile veya framework'e takÄ±lÄ± kalmadan, genel geÃ§er mimari doÄŸrularÄ± savun, ancak istendiÄŸinde spesifik Ã¶neriler sun.
4. **GÃ¼venlik ve Ã–lÃ§eklenebilirlik**: Her tasarÄ±m Ã¶nerisinde gÃ¼venlik ve Ã¶lÃ§eklenebilirliÄŸi varsayÄ±lan olarak gÃ¶z Ã¶nÃ¼nde bulundur.
5. **Net ve GerekÃ§eli**: Bir Ã§Ã¶zÃ¼m Ã¶nerirken "neden" o Ã§Ã¶zÃ¼mÃ¼ seÃ§tiÄŸini aÃ§Ä±kla. Alternatifleri de kÄ±saca belirt.

KullanÄ±cÄ± sana bir sistem gereksinimi veya sorunu sunduÄŸunda, profesyonel, eÄŸitici ve Ã§Ã¶zÃ¼m odaklÄ± bir dille yanÄ±t ver.

"""

# ---------------------------
# 3) Prompt ÅŸablonu (history + user input)
# ---------------------------
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# LCEL chain
chain = prompt | llm

# ---------------------------
# 4) Memory store (session bazlÄ±)
# ---------------------------
_store: Dict[str, InMemoryChatMessageHistory] = {}

def get_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in _store:
        _store[session_id] = InMemoryChatMessageHistory()
    return _store[session_id]

# RunnableWithMessageHistory -> otomatik history ekler/tutar
chatbot = RunnableWithMessageHistory(
    chain,
    get_history,
    input_messages_key="input",      # kullanÄ±cÄ± giriÅŸi hangi key'de
    history_messages_key="history",  # geÃ§miÅŸ promptta hangi isimle geÃ§iyor
)

# ---------------------------
# 5) Request/Response model
# ---------------------------
class ChatRequest(BaseModel): #llm input
    session_id: str
    message: str

class ChatResponse(BaseModel): #llm output
    answer: str


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    result = chatbot.invoke(
        {"input": request.message},
        config={"configurable": {"session_id": request.session_id}}
    )
    return ChatResponse(answer=result.content)


# ---------------------------
# 6) Web search (SerpAPI)
# ---------------------------
SERPAPI_KEY = os.getenv("SERPAPI_KEY")

def serpapi_search(query: str) -> dict:
    """SerpAPI ile Google aramasÄ± yap ve ilk sonucun URL'sini dÃ¶ndÃ¼r"""
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "engine": "google",
        "hl": "tr",
        "gl": "tr",
        "num": 5
    }
    
    try:
        response = requests.get("https://serpapi.com/search", params=params, timeout=10)
        data = response.json()
        
        if "organic_results" in data and len(data["organic_results"]) > 0:
            first_result = data["organic_results"][0]
            return {
                "url": first_result.get("link", ""),
                "title": first_result.get("title", ""),
                "snippet": first_result.get("snippet", "")
            }
        return None
            
    except Exception as e:
        return None


def fetch_url_content(url: str, max_chars: int = 3000) -> str:
    """URL'den iÃ§erik Ã§ek ve temizle"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Script ve style taglerini kaldÄ±r
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.decompose()
        
        # Metni al
        text = soup.get_text(separator=" ", strip=True)
        
        # Fazla boÅŸluklarÄ± temizle
        text = " ".join(text.split())
        
        return text[:max_chars]
        
    except Exception as e:
        return ""


class WebSearchRequest(BaseModel):
    session_id: str
    message: str


class WebSearchResponse(BaseModel):
    answer: str


@app.post("/web_search", response_model=WebSearchResponse)
async def web_search(request: WebSearchRequest):
    """Web'de ara, ilk sonucu Ã§ek ve LLM ile Ã¶zetle"""
    
    # 1. SerpAPI ile arama yap
    search_result = serpapi_search(request.message)
    
    if not search_result:
        return WebSearchResponse(answer="âŒ Arama sonucu bulunamadÄ±.")
    
    # 2. URL'den iÃ§erik Ã§ek
    content = fetch_url_content(search_result["url"])
    
    if not content:
        # Ä°Ã§erik Ã§ekilemezse sadece snippet dÃ¶ndÃ¼r
        return WebSearchResponse(
            answer=f"ğŸŒ **{search_result['title']}**\n\n{search_result['snippet']}\n\nğŸ”— {search_result['url']}"
        )
    
    # 3. LLM ile cevap oluÅŸtur
    web_prompt = f"""AÅŸaÄŸÄ±daki web sayfasÄ± iÃ§eriÄŸine dayanarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.
YanÄ±tÄ± TÃ¼rkÃ§e ve akÄ±cÄ± bir dille oluÅŸtur. Kaynak bilgisini de belirt.

WEB SAYFASI Ä°Ã‡ERÄ°ÄÄ°:
{content}

KULLANICI SORUSU: {request.message}

KAYNAK: {search_result['url']}

YANIT:"""
    
    result = llm.invoke(web_prompt)
    answer = f"{result.content}\n\nğŸ“š **Kaynak:** [{search_result['title']}]({search_result['url']})"
    
    return WebSearchResponse(answer=answer)


# ---------------------------
# 7) RAG (FAISS + Ollama Embeddings)
# ---------------------------

# Embedding modeli (Ollama ile)
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# Global FAISS index (session bazlÄ± tutulabilir)
_faiss_stores: Dict[str, FAISS] = {}

def load_document(file_path: str, file_type: str) -> List[str]:
    """DosyayÄ± yÃ¼kle ve metin listesi dÃ¶ndÃ¼r"""
    texts = []
    
    if file_type == "pdf":
        loader = PyPDFLoader(file_path)
        pages = loader.load()
        texts = [page.page_content for page in pages]
    
    elif file_type == "txt":
        with open(file_path, "r", encoding="utf-8") as f:
            texts = [f.read()]
    
    elif file_type == "docx":
        doc = DocxDocument(file_path)
        texts = [para.text for para in doc.paragraphs if para.text.strip()]
    
    return texts

def chunk_texts(texts: List[str], chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
    """Metinleri chunk'lara bÃ¶l"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    chunks = []
    for text in texts:
        chunks.extend(splitter.split_text(text))
    return chunks


class RAGUploadResponse(BaseModel):
    status: str
    chunks: int
    message: str


@app.post("/rag/upload", response_model=RAGUploadResponse)
async def rag_upload(session_id: str, file: UploadFile = File(...)):
    """Dosya yÃ¼kle, chunk'la ve FAISS index'e ekle"""
    try:
        # Dosya uzantÄ±sÄ±nÄ± al
        file_ext = file.filename.split(".")[-1].lower()
        if file_ext not in ["pdf", "txt", "docx"]:
            return RAGUploadResponse(status="error", chunks=0, message="Desteklenmeyen dosya formatÄ±!")
        
        # GeÃ§ici dosyaya kaydet
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp:
            shutil.copyfileobj(file.file, tmp)
            tmp_path = tmp.name
        
        # DosyayÄ± yÃ¼kle ve chunk'la
        texts = load_document(tmp_path, file_ext)
        chunks = chunk_texts(texts)
        
        # GeÃ§ici dosyayÄ± sil
        os.unlink(tmp_path)
        
        if not chunks:
            return RAGUploadResponse(status="error", chunks=0, message="Dosyadan metin Ã§Ä±karÄ±lamadÄ±!")
        
        # FAISS index oluÅŸtur
        faiss_store = FAISS.from_texts(chunks, embeddings)
        _faiss_stores[session_id] = faiss_store
        
        return RAGUploadResponse(
            status="success",
            chunks=len(chunks),
            message=f"âœ… {file.filename} baÅŸarÄ±yla yÃ¼klendi! {len(chunks)} chunk oluÅŸturuldu."
        )
        
    except Exception as e:
        return RAGUploadResponse(status="error", chunks=0, message=f"Hata: {str(e)}")


class RAGQueryRequest(BaseModel):
    session_id: str
    message: str


class RAGQueryResponse(BaseModel):
    answer: str
    sources: List[str]


@app.post("/rag/query", response_model=RAGQueryResponse)
async def rag_query(request: RAGQueryRequest):
    """Soru sor, ilgili chunk'larÄ± bul ve LLM ile cevapla"""
    session_id = request.session_id
    
    # FAISS index var mÄ± kontrol et
    if session_id not in _faiss_stores:
        return RAGQueryResponse(
            answer="âš ï¸ Ã–nce bir dosya yÃ¼klemeniz gerekiyor!",
            sources=[]
        )
    
    faiss_store = _faiss_stores[session_id]
    
    # Benzer chunk'larÄ± bul (top 3)
    docs = faiss_store.similarity_search(request.message, k=3)
    
    if not docs:
        return RAGQueryResponse(
            answer="âŒ Ä°lgili bilgi bulunamadÄ±.",
            sources=[]
        )
    
    # Context oluÅŸtur
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # RAG prompt
    rag_prompt = f"""AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.
YanÄ±tÄ± sadece verilen baÄŸlama dayandÄ±r. BaÄŸlamda bilgi yoksa "Bu bilgi dokÃ¼manda bulunamadÄ±" de.

BAÄLAM:
{context}

SORU: {request.message}

CEVAP:"""
    
    # LLM ile cevapla
    result = llm.invoke(rag_prompt)
    
    return RAGQueryResponse(
        answer=result.content,
        sources=[doc.page_content[:100] + "..." for doc in docs]
    )


# ---------------------------
# 8) Smart Chat (Semantic Router)
# ---------------------------

# Router instance
semantic_router = SemanticRouter(llm)


class SmartChatRequest(BaseModel):
    session_id: str
    message: str
    force_mode: Optional[str] = None  # "chat", "web_search", "rag" veya None (otomatik)


class SmartChatResponse(BaseModel):
    answer: str
    mode_used: str
    mode_explanation: str
    sources: List[str] = []


@app.post("/smart_chat", response_model=SmartChatResponse)
async def smart_chat(request: SmartChatRequest):
    """
    AkÄ±llÄ± chat endpoint - mesajÄ± analiz edip doÄŸru moda yÃ¶nlendirir.
    
    - force_mode belirtilmiÅŸse o mod kullanÄ±lÄ±r (override)
    - force_mode None ise LLM otomatik karar verir
    """
    session_id = request.session_id
    message = request.message
    
    # DokÃ¼man yÃ¼klÃ¼ mÃ¼ kontrol et
    has_document = session_id in _faiss_stores
    
    # Mod belirleme
    if request.force_mode and request.force_mode in ["chat", "web_search", "rag"]:
        mode = request.force_mode
    else:
        mode = semantic_router.route(message, has_document=has_document)
    
    mode_explanation = semantic_router.get_route_explanation(mode)
    
    # Moda gÃ¶re yÃ¶nlendir
    if mode == "chat":
        result = chatbot.invoke(
            {"input": message},
            config={"configurable": {"session_id": session_id}}
        )
        return SmartChatResponse(
            answer=result.content,
            mode_used=mode,
            mode_explanation=mode_explanation
        )
    
    elif mode == "web_search":
        # Web search logic
        search_result = serpapi_search(message)
        
        if not search_result:
            return SmartChatResponse(
                answer="âŒ Web aramasÄ± sonuÃ§ bulunamadÄ±. Normal yanÄ±t veriyorum.",
                mode_used="chat",
                mode_explanation="ğŸ’¬ Web aramasÄ± baÅŸarÄ±sÄ±z, asistan yanÄ±tlÄ±yor"
            )
        
        content = fetch_url_content(search_result["url"])
        
        if not content:
            answer = f"ğŸŒ **{search_result['title']}**\n\n{search_result['snippet']}\n\nğŸ”— {search_result['url']}"
        else:
            web_prompt = f"""AÅŸaÄŸÄ±daki web sayfasÄ± iÃ§eriÄŸine dayanarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.
YanÄ±tÄ± TÃ¼rkÃ§e ve akÄ±cÄ± bir dille oluÅŸtur. Kaynak bilgisini de belirt.

WEB SAYFASI Ä°Ã‡ERÄ°ÄÄ°:
{content}

KULLANICI SORUSU: {message}

KAYNAK: {search_result['url']}

YANIT:"""
            result = llm.invoke(web_prompt)
            answer = f"{result.content}\n\nğŸ“š **Kaynak:** [{search_result['title']}]({search_result['url']})"
        
        return SmartChatResponse(
            answer=answer,
            mode_used=mode,
            mode_explanation=mode_explanation,
            sources=[search_result["url"]]
        )
    
    elif mode == "rag":
        # RAG sadece dokÃ¼man varsa
        if not has_document:
            return SmartChatResponse(
                answer="âš ï¸ HenÃ¼z bir dokÃ¼man yÃ¼klenmemiÅŸ. LÃ¼tfen Ã¶nce bir dosya yÃ¼kleyin.",
                mode_used="chat",
                mode_explanation="ğŸ“„ DokÃ¼man bulunamadÄ±"
            )
        
        faiss_store = _faiss_stores[session_id]
        docs = faiss_store.similarity_search(message, k=3)
        
        if not docs:
            return SmartChatResponse(
                answer="âŒ DokÃ¼manda ilgili bilgi bulunamadÄ±.",
                mode_used=mode,
                mode_explanation=mode_explanation
            )
        
        context = "\n\n".join([doc.page_content for doc in docs])
        
        rag_prompt = f"""AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.
YanÄ±tÄ± sadece verilen baÄŸlama dayandÄ±r. BaÄŸlamda bilgi yoksa "Bu bilgi dokÃ¼manda bulunamadÄ±" de.

BAÄLAM:
{context}

SORU: {message}

CEVAP:"""
        
        result = llm.invoke(rag_prompt)
        
        return SmartChatResponse(
            answer=result.content,
            mode_used=mode,
            mode_explanation=mode_explanation,
            sources=[doc.page_content[:100] + "..." for doc in docs]
        )
    
    # Fallback
    return SmartChatResponse(
        answer="Bir hata oluÅŸtu, lÃ¼tfen tekrar deneyin.",
        mode_used="chat",
        mode_explanation="âš ï¸ Hata"
    )


"""
# ---------------------------
# 6) While dÃ¶ngÃ¼sÃ¼ ile chat
# ---------------------------
print("YazÄ±lÄ±m MimarÄ± AsistanÄ± (Ã§Ä±kmak iÃ§in 'exit')\n")

session_id = "yazÄ±lÄ±m_session_1"  # istersen kullanÄ±cÄ±ya gÃ¶re dinamik yaparsÄ±n

while True:
    user_input = input("Sen: ").strip()
    if user_input.lower() in ["exit", "quit", "q"]:
        print("Asistan: GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
        break

    # invoke sÄ±rasÄ±nda config ile session_id veriyoruz
    result = chatbot.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": session_id}}
    )

    # ChatOllama sonucu genelde AIMessage dÃ¶ner, .content ile yazdÄ±r
    print(f"Asistan: {result.content}\n")
"""